# CommonForge: AI-Powered B2B Lead Scoring

An intelligent lead scoring system that analyzes B2B companies using AI to generate ICP (Ideal Customer Profile) fit scores, segment classifications, and personalized sales insights.

## ğŸ¯ What It Does

CommonForge takes a list of companies and:

1. **Loads** structured data from CSV (name, domain, country, size, industry)
2. **Scrapes** company websites to gather unstructured content
3. **Analyzes** using AI (GPT-4) to score and classify each company
4. **Stores** enriched data in PostgreSQL
5. **Serves** insights via REST API with advanced filtering

**Example Output:**
- **ICP Fit Score:** 92/100
- **Segment:** Enterprise
- **Use Case:** "Sales automation for high-velocity teams"
- **Risk Flags:** []
- **Pitch:** "As a fast-growing B2B SaaS company, you're the perfect fit for our enterprise sales engagement platform..."

## âœ¨ Features

- **Web UI:** Interactive Vue.js app for uploading files and viewing results
- **Multi-Source Data Ingestion:** CSV + async web scraping
- **AI-Powered Enrichment:** LLM-based analysis using LangChain + GPT-4
- **Intelligent Scoring:** 0-100 ICP fit score based on 20+ factors
- **Segment Classification:** Automatic SMB/Mid-Market/Enterprise categorization
- **Personalized Insights:** Custom sales pitches and risk flags for each company
- **REST API:** FastAPI-powered API with filtering, pagination, and sorting
- **Production Ready:** 53 tests passing, 66% coverage, type hints throughout

## ğŸ›  Tech Stack

**Backend:**
- **Language:** Python 3.11+
- **Web Framework:** FastAPI
- **Database:** PostgreSQL 14+
- **ORM:** SQLAlchemy 2.0
- **LLM Framework:** LangChain
- **LLM Provider:** OpenAI GPT-4
- **HTTP Client:** HTTPX (async)
- **HTML Parsing:** BeautifulSoup4
- **Data Processing:** Pandas
- **Testing:** pytest + pytest-asyncio

**Frontend:**
- **Framework:** Vue 3 (via CDN)
- **Styling:** Tailwind CSS
- **Charts:** Chart.js
- **Architecture:** Single-page app (SPA)

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+** installed
- **PostgreSQL 14+** running locally or remotely
- **OpenAI API key** from [platform.openai.com](https://platform.openai.com/api-keys)

### Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/getSweetSpotcl/CommonForge.git
   cd CommonForge
   ```

2. **Create and activate virtual environment:**

   ```bash
   python3.11 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

4. **Configure environment variables:**

   ```bash
   cp .env.example .env
   ```

   Edit `.env` with your credentials:

   ```bash
   # Required
   DATABASE_URL=postgresql+psycopg2://postgres:yourpassword@localhost:5432/commonforge
   OPENAI_API_KEY=sk-proj-your-actual-key-here

   # Optional (defaults are fine)
   OPENAI_MODEL=gpt-4-turbo-preview
   SCRAPER_TIMEOUT=10
   MAX_WEBSITE_TEXT_LENGTH=3000
   ```

5. **Set up PostgreSQL database:**

   ```bash
   # Create database
   createdb commonforge

   # Initialize tables (automatically done on first pipeline run)
   python -c "from src.db import init_db; init_db()"
   ```

6. **Verify setup:**

   ```bash
   # Test configuration
   python -c "from src.config import settings; print('âœ… Config loaded')"

   # Test database connection
   python -c "from src.db import check_connection; print('âœ… DB Connected' if check_connection() else 'âŒ DB Failed')"
   ```

---

## ğŸ“– Usage Guide

### Option 1: Run the Complete Pipeline

The pipeline processes companies end-to-end: CSV â†’ Scraping â†’ AI Enrichment â†’ Database

#### Basic Usage

```bash
# Process all companies from sample data
./scripts/run_pipeline.sh data/companies.csv
```

**What it does:**
1. Loads 5 companies from CSV
2. Scrapes their websites (async, with retries)
3. Analyzes with GPT-4 (ICP scoring, segmentation, pitches)
4. Saves enriched data to PostgreSQL
5. Shows summary statistics

**Output example:**
```
ğŸ“„ STEP 1: Loading CSV Data
âœ“ Loaded 5 companies from CSV

ğŸŒ STEP 2: Scraping Websites
âœ“ Scraped 5 websites - 4 successful (80.0%)

ğŸ”„ STEP 3: Merging Data
âœ“ Merged 5 company records

ğŸ¤– STEP 4: LLM Enrichment
âœ“ Enriched 4 companies - 4 successful (100.0%)

ğŸ’¾ STEP 5: Persisting to Database
âœ“ Persisted 5 companies to database

âœ… PIPELINE COMPLETED SUCCESSFULLY
```

#### Advanced Options

```bash
# Dry run (test without saving to database)
./scripts/run_pipeline.sh data/companies.csv --dry-run

# Process only first 2 companies (save API costs)
./scripts/run_pipeline.sh data/companies.csv --max-companies 2

# Skip website scraping (use only CSV data)
./scripts/run_pipeline.sh data/companies.csv --skip-scraping

# Skip LLM enrichment (test scraping only)
./scripts/run_pipeline.sh data/companies.csv --skip-enrichment

# Combine options
./scripts/run_pipeline.sh data/companies.csv --dry-run --max-companies 3
```

#### Using Your Own Data

Create a CSV file with these required columns:

```csv
company_name,domain,country,employee_count,industry_raw
Acme Corp,acme.com,USA,500,SaaS
TechStart,techstart.io,UK,50,Software
```

Then run:

```bash
./scripts/run_pipeline.sh path/to/your/companies.csv
```

---

### Option 2: Use the REST API

Query enriched company data via REST API with filtering, pagination, and sorting.

#### Start the API Server

```bash
./scripts/run_api.sh
```

Server starts at `http://localhost:8000`

#### Interactive Documentation

- **Swagger UI:** http://localhost:8000/docs (try it out interactively)
- **ReDoc:** http://localhost:8000/redoc (clean documentation)

#### API Endpoints

**1. Health Check**

```bash
# Basic health
curl http://localhost:8000/

# Detailed health with database stats
curl http://localhost:8000/health
```

**2. List Companies**

```bash
# Get all companies
curl http://localhost:8000/companies

# With pagination
curl "http://localhost:8000/companies?skip=0&limit=10"

# Filter by country
curl "http://localhost:8000/companies?country=USA"

# Filter by segment (SMB, Mid-Market, Enterprise)
curl "http://localhost:8000/companies?segment=Enterprise"

# Filter by ICP score range
curl "http://localhost:8000/companies?min_score=80&max_score=100"

# Only show enriched companies
curl "http://localhost:8000/companies?enriched_only=true"

# Combine filters
curl "http://localhost:8000/companies?segment=Enterprise&min_score=85&country=USA"

# Sort by score (descending - highest first)
curl "http://localhost:8000/companies?sort_by=icp_fit_score&sort_order=desc"

# Sort by name
curl "http://localhost:8000/companies?sort_by=company_name&sort_order=asc"
```

**3. Get Single Company**

```bash
# By ID
curl http://localhost:8000/companies/1

# By domain
curl http://localhost:8000/companies/by-domain/hubspot.com
```

**4. Get Statistics**

```bash
curl http://localhost:8000/stats
```

Returns:
- Total and enriched company counts
- Enrichment success rate
- Average ICP score
- Breakdown by segment (SMB/Mid-Market/Enterprise)
- Breakdown by country
- Top 10 highest-scoring companies

---

### Option 3: Use the Web UI

**NEW!** Interactive web application for uploading files and viewing results.

#### Start the Application

```bash
# Start both backend and frontend
./scripts/start.sh
```

Or start manually:

```bash
# Backend API
./scripts/run_api.sh

# Then open in browser
open http://localhost:8000/app/
```

#### Features

**Upload View:**
- Drag & drop CSV files
- Preview data before processing
- Upload and trigger pipeline

**Processing View:**
- Real-time progress bar
- Step-by-step status updates
- Auto-navigate when complete

**Companies View:**
- Sortable table (click column headers)
- Multi-filter: search, country, segment, score
- Color-coded ICP scores
- Pagination
- Click row for full details

**Statistics Dashboard:**
- Metric cards (total, enriched, avg score)
- Segment distribution pie chart
- Country distribution bar chart
- Top 10 companies by score

#### Quick Demo

1. Go to http://localhost:8000/app/
2. Click "Upload" tab
3. Drag & drop `data/companies.csv`
4. Watch real-time progress
5. View enriched results!

See [FRONTEND-GUIDE.md](FRONTEND-GUIDE.md) for detailed usage.

---

### Option 4: Programmatic Python API

Use CommonForge components directly in your Python code.

```python
import asyncio
from pathlib import Path
from src.ingestion.structured import load_companies_from_csv
from src.ingestion.unstructured import scrape_companies
from src.processing.cleaning import merge_structured_unstructured, prepare_for_enrichment
from src.processing.llm_chain import enrich_companies

async def process_companies():
    # 1. Load CSV
    companies = load_companies_from_csv(Path('data/companies.csv'))
    print(f"Loaded {len(companies)} companies")

    # 2. Scrape websites
    domains = [c['domain'] for c in companies]
    scraped = await scrape_companies(domains)
    print(f"Scraped {len(scraped)} websites")

    # 3. Merge structured + unstructured data
    merged = merge_structured_unstructured(companies, scraped)

    # 4. Prepare for enrichment (filter companies with website data)
    ready = prepare_for_enrichment(merged)
    print(f"{len(ready)} companies ready for enrichment")

    # 5. Enrich with AI
    enriched = await enrich_companies(ready)

    # 6. View results
    for company, enrichment in zip(ready, enriched):
        print(f"\n{company['company_name']}:")
        print(f"  Score: {enrichment['icp_fit_score']}/100")
        print(f"  Segment: {enrichment['segment']}")
        print(f"  Pitch: {enrichment['personalized_pitch']}")

    return enriched

# Run
results = asyncio.run(process_companies())
```

**Query Database Directly:**

```python
from src.db import SessionLocal
from src.models import Company

db = SessionLocal()

# Get all high-scoring Enterprise companies
results = db.query(Company).filter(
    Company.segment == 'Enterprise',
    Company.icp_fit_score >= 80
).all()

for company in results:
    print(f"{company.company_name}: {company.icp_fit_score}/100")

db.close()
```

---

## ğŸ”§ Development

### Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# View coverage report
open htmlcov/index.html

# Run specific test file
pytest tests/test_api.py -v

# Run specific test
pytest tests/test_api.py::TestHealthEndpoints::test_health_check_endpoint -v
```

**Test Results:**
- âœ… 53 tests passing
- âŒ 0 tests failing
- ğŸ“Š 66% code coverage

### Code Quality

```bash
# Format code with black
black src/ tests/

# Lint with flake8
flake8 src/ --max-line-length=100 --ignore=E203,W503

# Type checking (optional)
mypy src/
```

### Project Structure

```text
CommonForge/
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ api/                   # FastAPI REST API
â”‚   â”‚   â””â”€â”€ main.py           # API endpoints, file upload, job tracking
â”‚   â”œâ”€â”€ ingestion/            # Data ingestion layer
â”‚   â”‚   â”œâ”€â”€ structured.py     # CSV loading & validation
â”‚   â”‚   â””â”€â”€ unstructured.py   # Website scraping
â”‚   â”œâ”€â”€ processing/           # LLM enrichment
â”‚   â”‚   â”œâ”€â”€ cleaning.py       # Data merging & preparation
â”‚   â”‚   â””â”€â”€ llm_chain.py      # LangChain + GPT-4 integration
â”‚   â”œâ”€â”€ config.py             # Pydantic Settings
â”‚   â”œâ”€â”€ db.py                 # SQLAlchemy database connection
â”‚   â”œâ”€â”€ models.py             # ORM models
â”‚   â”œâ”€â”€ schemas.py            # Pydantic schemas
â”‚   â””â”€â”€ pipeline.py           # Main orchestration
â”œâ”€â”€ frontend/                  # Web UI (Vue 3 + Tailwind)
â”‚   â””â”€â”€ index.html            # Single-page application
â”œâ”€â”€ tests/                     # Test suite (pytest)
â”‚   â”œâ”€â”€ test_api.py           # API endpoint tests
â”‚   â”œâ”€â”€ test_ingestion.py     # CSV & scraping tests
â”‚   â”œâ”€â”€ test_processing.py    # LLM enrichment tests
â”‚   â”œâ”€â”€ test_pipeline_e2e.py  # End-to-end tests
â”‚   â””â”€â”€ test_full_ingestion.py
â”œâ”€â”€ scripts/                   # Helper scripts
â”‚   â”œâ”€â”€ start.sh              # Start frontend + backend
â”‚   â”œâ”€â”€ run_pipeline.sh       # Pipeline runner
â”‚   â”œâ”€â”€ run_api.sh            # API server starter
â”‚   â””â”€â”€ init_db.sh            # Database initialization
â”œâ”€â”€ data/                      # Sample data
â”‚   â””â”€â”€ companies.csv         # 5 sample companies
â”œâ”€â”€ plans/                     # Implementation plans
â”‚   â”œâ”€â”€ frontend-vue-tailwind.md
â”‚   â””â”€â”€ ...                   # Detailed phase plans
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ pytest.ini                # Test configuration
â”œâ”€â”€ PROJECT-PROGRESS.md       # Development progress
â”œâ”€â”€ FRONTEND-GUIDE.md         # Frontend usage guide
â””â”€â”€ README.md                 # This file
```

---

## ğŸ› Troubleshooting

### Database Connection Issues

**Error:** `Connection refused` or `could not connect to server`

```bash
# Check if PostgreSQL is running
pg_isready

# Start PostgreSQL (macOS with Homebrew)
brew services start postgresql@14

# Start PostgreSQL (Linux)
sudo systemctl start postgresql

# Verify DATABASE_URL in .env is correct
echo $DATABASE_URL
```

### OpenAI API Errors

**Error:** `AuthenticationError` or `Invalid API key`

```bash
# Verify API key in .env
grep OPENAI_API_KEY .env

# Test API key
python -c "from openai import OpenAI; OpenAI(api_key='YOUR_KEY').models.list()"
```

**Error:** `Rate limit exceeded`

- Use `--max-companies 2` to process fewer companies
- Add delays between requests (already implemented)
- Upgrade OpenAI plan for higher rate limits

### Import Errors

**Error:** `ModuleNotFoundError: No module named 'src'`

```bash
# Ensure virtual environment is activated
source venv/bin/activate

# Reinstall dependencies
pip install -r requirements.txt

# Run from project root directory
pwd  # Should show .../CommonForge
```

### Website Scraping Fails

**Error:** `Failed to scrape after multiple attempts`

- Some websites block scrapers (expected behavior)
- Pipeline continues with available data
- Use `--skip-scraping` to test without scraping
- Check website is accessible: `curl https://example.com`

### Pipeline Hangs

- OpenAI API calls can take 5-10 seconds per company
- Check logs for progress
- Use `--max-companies 2` for faster testing
- Verify API key has sufficient credits

---

## ğŸ“Š Performance & Costs

### Pipeline Performance

For 5 companies (sample data):
- **CSV Loading:** < 1 second
- **Website Scraping:** 10-30 seconds (async, parallel)
- **LLM Enrichment:** 20-50 seconds (batch processing)
- **Database Persistence:** < 1 second
- **Total Runtime:** ~1-2 minutes

### OpenAI API Costs

Approximate costs per company (GPT-4 Turbo):
- **Input tokens:** ~500-1000 tokens ($0.01 per 1K)
- **Output tokens:** ~200-400 tokens ($0.03 per 1K)
- **Cost per company:** ~$0.01-0.02
- **100 companies:** ~$1-2

Use `--dry-run` and `--max-companies 2` for cost-free testing.

---

## ğŸ— Architecture

### Data Flow

```
CSV File â†’ Load â†’ Validate â†’ Normalize
                                â†“
                          Scrape Websites (async)
                                â†“
                          Merge Structured + Unstructured
                                â†“
                          Enrich with GPT-4
                                â†“
                          PostgreSQL Database
                                â†“
                          FastAPI REST API
```

### Key Design Decisions

- **Async I/O:** HTTPX for concurrent website scraping
- **Batch Processing:** LLM calls batched with rate limiting
- **Upsert Logic:** Updates existing companies (by domain)
- **Type Safety:** Full type hints + Pydantic validation
- **Error Recovery:** Pipeline continues despite individual failures
- **Modular Design:** Each phase independently testable

See [plan/01-architecture.md](plan/01-architecture.md) for detailed architecture.

---

## ğŸ“ Environment Variables

### Required

| Variable | Description | Example |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection string | `postgresql+psycopg2://user:pass@localhost:5432/commonforge` |
| `OPENAI_API_KEY` | OpenAI API key | `sk-proj-abc123...` |

### Optional

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_MODEL` | GPT model to use | `gpt-4-turbo-preview` |
| `OPENAI_TEMPERATURE` | LLM temperature (0-1) | `0.3` |
| `OPENAI_MAX_TOKENS` | Max tokens per response | `1000` |
| `SCRAPER_TIMEOUT` | HTTP timeout (seconds) | `10` |
| `SCRAPER_MAX_RETRIES` | Retry attempts | `3` |
| `SCRAPER_USER_AGENT` | HTTP User-Agent | `CommonForge Lead Scorer/1.0` |
| `MAX_WEBSITE_TEXT_LENGTH` | Max scraped text length | `3000` |
| `CONCURRENT_LLM_CALLS` | Parallel LLM requests | `3` |
| `API_HOST` | API server host | `0.0.0.0` |
| `API_PORT` | API server port | `8000` |
| `LOG_LEVEL` | Logging level | `INFO` |

---

## ğŸ¤ Contributing

This is a demonstration project. For questions or issues:

1. Check [PROJECT-PROGRESS.md](PROJECT-PROGRESS.md) for implementation details
2. Review [CLAUDE.md](CLAUDE.md) for development guide
3. Check test files for usage examples

---

## ğŸ“„ License

MIT License - for evaluation and demonstration purposes.

---

## ğŸ‘¤ Author

**Gustavo CortÃ­nez**
Principal AI Engineer Candidate

**Repository:** https://github.com/getSweetSpotcl/CommonForge

**Project Stats:**
- 3,663 lines of code (832 production, 1,546 tests)
- 53 tests passing (66% coverage)
- 7 phases completed in ~3.75 hours
- Production-ready and fully functional

---

## ğŸ¯ Next Steps

After setup, try these:

1. **Start the Web UI (Recommended):**
   ```bash
   ./scripts/start.sh
   # Open http://localhost:8000/app/
   ```

2. **Or run CLI pipeline:**
   ```bash
   ./scripts/run_pipeline.sh data/companies.csv --max-companies 2
   ```

3. **Or use the REST API:**
   ```bash
   ./scripts/run_api.sh
   # Open http://localhost:8000/docs
   ```

4. **Query enriched data:**
   ```bash
   curl "http://localhost:8000/companies?segment=Enterprise"
   ```

5. **Process your own data:**
   - Create CSV with required columns
   - Upload via web UI or run pipeline
   - View results in web UI or query via API

**Questions?** Check the documentation files or review the test suite for examples.

---

**Built with â¤ï¸ using Claude Code**
